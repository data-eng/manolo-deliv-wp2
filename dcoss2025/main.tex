\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote
%Template version as of 6/27/2024

% \usepackage{cite}
\usepackage[noadjust]{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}

\usepackage{booktabs} % for professional tables
\usepackage{colortbl}
\usepackage{subcaption}
\usepackage{multirow}

\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[colorlinks=true, linkcolor=black, urlcolor=blue, citecolor=black]{hyperref}
\usepackage{array}

\newcommand{\tbfs}[3]{\textbf{#3\%}(#1\%,#2\%)}
\newcommand{\tbms}[3]{\hskip -0.2cm \textbf{#3\%}(#1\%,#2\%)}
\newcommand{\tbmv}[3]{\hskip -0.5cm \textbf{#3\%}(#1\%,#2\%)}
\newcommand{\tbmw}[3]{\hskip -0.6cm \textbf{#3\%}(#1\%,#2\%)}
\newcommand{\tbmm}[3]{\hskip -0.9cm \textbf{#3\%}(#1\%,#2\%)}
\newcommand{\tbq}{0.3cm}

\DeclareUnicodeCharacter{2212}{-}


% \def\anon{1}
\def\anon{0}


\begin{document}

\title{Investigation of Attention as a predictor \\
 for stream-data anomalies\thanks{Funding Acknowledgment in final version.}
}

\ifnum\anon=0

\author{\IEEEauthorblockN{Natalia Koliou}
\IEEEauthorblockA{\textit{Institute of Informatics and} \\
\textit{Telecomm., NCSR Demokritos}\\
Ag. Paraskevi, Greece \\
0009-0004-3920-9992}
\and
\IEEEauthorblockN{Georgios Dimitriou}
\IEEEauthorblockA{\textit{R\&D Department} \\
\textit{Four Dot Infinity}\\
Chalandri, Greece \\
0009-0006-7227-9612}
\and
\IEEEauthorblockN{Maria Sierra}
\IEEEauthorblockA{\textit{Data Science Department} \\
\textit{BitBrain}\\
Zaragoza, Spain \\
maria.sierra@bitbrain.com}
\and
\IEEEauthorblockN{Christoforos Romesis}
\IEEEauthorblockA{\textit{Institute of Informatics and} \\
\textit{Telecomm., NCSR Demokritos}\\
Ag. Paraskevi, Greece \\
0009-0001-6485-5548}
\and
\IEEEauthorblockN{\hskip 5em Stasinos Konstantopoulos}
\IEEEauthorblockA{\textit{\hskip 5em Institute of Informatics and} \\
\textit{\hskip 5em Telecomm., NCSR Demokritos}\\
\hskip 5em Ag. Paraskevi, Greece \\
\hskip 5em 0000-0002-2586-1726}
\and
\IEEEauthorblockN{Panagiotis Trakadas}
\IEEEauthorblockA{\textit{R\&D Department} \\
\textit{Four Dot Infinity}\\
Chalandri, Greece \\
0000-0002-5146-5954}
\and
\IEEEauthorblockN{Luis Montesano}\hfill
\IEEEauthorblockA{\textit{Chief Scientific Officer} \\
\textit{BitBrain}\\
Zaragoza, Spain \\
luis.montesano@bitbrain.com}\hfill}

\else

\author{\IEEEauthorblockN{Authors}
\IEEEauthorblockA{\textit{Affiliations}}}

\fi

\maketitle

\begin{abstract}
This paper presents a novel approach that employs attention mechanisms as a key indicator for detecting anomalies within time-series data. Machine learning models such as Autoencoders and Transformers are utilized for the tasks of signal reconstruction and sequence prediction, respectively. Autoencoders are designed to reconstruct input data, identifying noise through reconstruction errors; larger errors signify a higher likelihood of noise presence in the data segment. Conversely, Transformers are utilized to predict subsequent data points in a sequence, with noise identified through prediction errors. Building on these techniques, the proposed approach uses attention layers within machine learning models to estimate noise by assuming that lower attention values indicate noisy or, more generally, inconsistent data. The proposed method was evaluated for its effectiveness in detecting anomalies in electroencephalogram (EEG) signals acquired from medical-grade wearable devices. In this study, EEG signals are utilized to identify the sleep stages of subjects while they are sleeping. Each sleep stage is associated with distinct brain activity patterns characterized by specific waveforms. However, EEG signals collected from wearable devices during sleep are susceptible to noise, which complicates the accurate identification of contaminated data segments. The performance of the proposed approach was assessed against the established ground truth method used by the wearable manufacturer and other state-of-the-art baseline anomaly detection methods. The results indicate that the proposed method serves as a promising task-agnostic tool for anomaly detection in streaming data, particularly when compared to conventional baseline techniques.
\end{abstract}

\begin{IEEEkeywords}
AI, Machine learning, Transformers, Robustness, Time-Series, anomaly detection, noise estimation, outlier detection.
\end{IEEEkeywords}


\section{Introduction}

The term \emph{anomaly} is used in time series processing to indicate
a wide range of deviations and encompasses not only unforeseen events
in the observed system but also transient problems in the obervation
mechanism. Indicatively, across various domains, an anomaly in the
observation data can be attributed to a cyberattack, a disease, a
faulty subsystem or, on the other hand, to a sensor momentarily
loosing contact or being heavily affected by noise in the environment
\cite{laptev_generic_2015,geiger_tadgan:_2020,ji_novel_2021,zhou_anomaly_2021}.

This vague definition of what constitutes an anomaly brings with it
the range of technical challenges that distinctively characterize the
field: the long and complex temporal dependencies that need to be
considered due to the priorly unknown duration of what constitutes an
event, non-stationarity when aiming at correctly separating anomalies
from data shifts due to long-term trends, high-dimensional data as
there is no clear prior knowledge of what variables are needed.
As conventional approaches often struggle to provide satisfactory
results, or are limited by the need to tune their parameters for each
specific application on a case-by-case basis, recent advancements in
\emph{deep learning} have gained significant attention in the research
community \cite{MEJRI2024124922}. And, given the vagueness of what
constitutes ground truth, the most successful approaches are
unsupervised or self-supervised: \emph{forecasting-based} approaches
train a neural network to predict the next datapoint in the sequence
and \emph{reconstruction-based} approaches train a neural network to
reconstruct the sequence from a learned, compressed representation.
In either case, the assumption is that a high error between the
predicted/reconstructed sequence and the actual data is indicative of
an abnormality.

However, as Mejri et al. \cite{MEJRI2024124922} note, there is no
general-purpose approach (or even paradigm) that works best across
domains and applications and performance depends on the use case and
the nature of the encountered anomalies; Deep learning methods appear
in general superior, but conventional approaches (mostly PCA) can also
in some cases perform. To adress this, in this article we explore a
novel idea that aims to involve the use case into the training of the
anomaly detection model in a way that the system \emph{adapts itself}
to different use cases. In other words, instead of aiming at
general-purpose anomaly detector, we aim at a general-purpose way to
train a case-specific anomaly detector without direct supervision.

The core of the idea is that anomaly detection is a pre-processing
step for some sequence processing task. We operate under the
assumption that there is supervision for this downstream task,
although there is no supervision for what constitutes an anomaly.
We then make the following reseach hypothesis:
\emph{Anomalies are the parts of the sequence that have the property
that ignoring them gives superior performance despite the fact that
decisions are made from fewer datapoints.}

One can easily see how this maps directly to specific instances of
anomalies. For example, assuming a classification task on sensor data,
a burst of environment noise looks the same regardless of the correct
class during that period; a successful classifier would learn to
ignore such bursts as the same pattern maps to any class. As another
example, a successful model for predicting upcoming network traffic
would learn to ignore some kinds of bursts (although they might
indicate a cyberattack) and leanr to emphesise `normal' bursts
(that indicate, for instance, the start of working hours).

Our contribution is the formulation of a methodology for leveraging
the outputs of intermediate layers of deep neural networks in order
to extract the level of significance the network places on the
different parts of the sequence being learnt. In the remainder of
this article, we first provide the necessary background
(Section~\ref{sec:bg}) and then proceed to describe our
methodology (Section~\ref{sec:method}), which we evaluate on
an EEG dataset where we have supervision for both a downstream task
(sleep stage prediction) \emph{and} carefully curated anomaly
annotations (Section~\ref{sec:exp}). We then present and discuss
the emperiment results (Section~\ref{sec:results}) and conclude
(Section~\ref{sec:conc}).



%%%%%%%%%%%%
\section{Backrgound}
\label{sec:bg}
%%%%%%%%%%%%


One of the main frameworks in sequence processing are
\emph{recurrent neural networks} where the sequence is presented to
the network one token at a time and the network maintains a
\emph{hidden state} which distills the information needed from past
tokens to provide a context for the processing of the current token.
One of the most successful recurrent architectures is the
\emph{Long Short-Term Memory (LSTM)} where trainable \emph{gates}
control the flow of information to and from the hidden state
\cite{hochreiter-schmidhuber:1997}. This allows LSTM to capture long
temporal dependencies in a low-dimensional state representation
\cite{HOJJATI2024106106,10744017}.

LSTMs are often combined with other techniques, such as convolution
and encoder-decoder architectures. Encoder-decoder LSTMs, in
particular, learn a compressed representation of the data. When
training the decoder to reconstruct the original sequence back
(which is known as \emph{autoencoding}), the compressive encoding is
trained to drop information that is not detrimental for the loss
estimation. Anomalies (in the sense of patterns not encountered during
training) can then be identified by higher reconstruction error
\cite{SGUEGLIA2022170,aerospace6110117}.
Reconstruction error over autoencoding has been used extensively for
anomaly detection in various architecture besides LSTM
\cite{10.1145/3691338}, and its limitations are well-understood:
Autoencoders struggle with high-dimensional data, as it is up to the
system designer to find the layer widths (effectively, the level of
compression) that drop the correct amount of information.
Further, autoencoders are limited with the respect to the kind of
information they drop: Since this decision is driven by a loss that
compares the reconstructed sequences against the input sequence, a
commonly occuring pattern will be retained even when it is an anomaly
in a given context.

Besides recurency, the other major approach to sequence-processing is
the \emph{Transformer} architecture. Transformers are autoencoders
based on the \emph{self-attention} mechanism \cite{vaswani-etal:2017}.
Unlike LSTMs and other recurrent architectures, Transformers receive
the complete sequence as input and model relationships across the
entire sequence. This allows gradients to flow directly across the
entire sequence, rather than being propagated step by step, making it
easier to discover long-distance dependencies.
%
At the core of the Transformer architecture lies the self-attention
mechanism, which enables the model to assign different levels of
importance to elements in the input sequence
based on the value of \emph{any other element in the sequnce.}
Specifically, Transformers learn three sets of weights which are
applied to the input to get three vectors: the \emph{query (Q)}, the
\emph{key (K)}, and the \emph{Value (V)}.
The attention mechanism computes the \emph{similarity score} $QK^T$
which is a $n\times n$ matrix that determines the contribution of each
element in the final representation of each other element. The
similarity score is scaled and softmax'ed into a matrix of weights,
which are applied to the value vector. This yields the representation
%
$\mathrm{softmax}\left(QK^T / \mathrm{sqrt}(d_k)\right)\cdot V$
%
where each element contains information aggregated from the entire
sequence, improving the model's ability to detect long-range dependencies.



%%%%%%%%%%%%
\section{Research Methodology}
\label{sec:method}
%%%%%%%%%%%%

\subsection{Research Hypothesis}

As discussed in the previous section, reconstruction and prediction
errors are widely used as key indicators for anomaly detection in
time-series data due to their intuitive appeal and straightforward
implementation. However, these approaches do not take into account
differences in the nature of what is considered an anomaly for each
use case.

As we framed our work in a context where there is no anomaly
supervision, it follows that it is also not possible to select
training data that is not contaminated with anomalies. This means that
the model may inadvertently learn to reconstruct anomalies.
Additionally, reconstruction-based methods often struggle to detect
contextual anomalies, where an observation may be anomalous only in
specific temporal or multivariate contexts. For example, a
high-temperature reading might be expected in the summer but anomalous
in the winter, and reconstruction models may overlook such contextual
nuances.

Similarly, prediction error, which measures deviations between
predicted and actual values, can be susceptible to noise and
non-stationarity in time-series data. In highly dynamic systems,
normal variations may result in significant prediction errors, leading
to false positives. Moreover, models relying on prediction error often
assume that future patterns can be reliably forecasted based on past
observations, an assumption that may not hold in volatile or chaotic
systems.

On the other hand, the attention mechanism offers an alternative,
currently unexplored, way to extract indications about what parts of
the sequence are anomalies. The hypothesis is that the part of the
sequence that receives the least attention while performing a task
relevant to the use case, then is this part is an anomaly. The
intuition is that neural networks will happily overfit the data
when they are given enough parameters to do so. Such a network might
not be good to actually perform the task, but is good at recognizing
two ways in which a sub-sequence is anomalous: (a) it does not follow
any pattern that was boosted (gradient-wise) by the training;
(b) it follows a pattern that is inconsistent, it is sometimes
associated with one class on the task and sometimes with an another,
so it alternates between being boosted and penalized by the loss
function, again resulting in low attention.

To investigate this hypothesis, we experimented with several methods
for anomaly and noise detection ranging from attention-based methods
to state-of-the-art machine learning approaches, as well as
conventional anomaly detection methods. These are listed in
Table~\ref{tab1} and described in more detail in the remainder of this
sectiom.


\begin{table}[bt]
\centering
\caption{Methods under comparison.}
\label{tab1}
\begin{tabular}{lp{4cm}p{2cm}}
\toprule
\emph{Acronym}   & \emph{Architecture}& \emph{Detection}  \\
\midrule
\textit{LSTM}    & LSTM Autoencoder   & Reconstruction  \\
\textit{C-LSTM}  & Convolutional LSTM Autoencoder
                                      & error  \\
\textit{AE\_err} & Attention-based Autoencoder
                                      &   \\
\midrule
\textit{TP\_err} & Transformer Predictor
                                      & Prediction error  \\
\midrule
\textit{AE\_att} & Attention-based Autoencoder
                                      & Attention \\
\textit{TP\_att} & Transformer Predictor &  \\
\midrule
\textit{PCA}     & \multicolumn{2}{l}{Principle component analysis} \\
\textit{MNE}     & \multicolumn{2}{l}{IIR filter} \\
\bottomrule
\end{tabular} 
\end{table}


\subsection{Reconstruction and Prediction Error}

The \emph{LSTM Autoencoder (LSTM)} uses a sequence-to-sequence
architecture with LSTM layers for both encoding and decoding. The
encoder compresses the input into a fixed-size latent representation
by processing the input sequence and retaining the final hidden state
of the LSTM. This is achieved using an LSTM layer, followed by a
dropout layer for regularization, and a fully-connected layer for
dimensionality reduction. The decoder then processes this compressed
representation using the reverse architecture (FC, dropout, and LSTM)
to reconstruct the original data.

In this architecture, fully connected (FC) layers process the final
hidden state of the LSTM (the last timestep in the sequence) as a
single, comprehensive representation of the entire input
sequence. This output is then projected into a lower-dimensional
latent space through a linear transformation. While this approach is
computationally efficient, it assumes that the LSTM's final hidden
state sufficiently captures all relevant temporal dependencies. As a
result, it can struggle to retain fine-grained temporal details,
especially for data like EEG signals, where localized patterns are
crucial.

The \emph{Convolutional LSTM Autoencoder (C-LSTM)} enhances feature
extraction by combining LSTM layers with convolutional layers. The
key difference between the LSTM and C-LSTM architectures lies in how
dimensionality reduction is achieved in the encoder and decoder: the
first uses fully connected (FC) layers, while the latter uses
Conv1D layers. These operate directly on the sequence of hidden states
produced by the LSTM. By applying a kernel across the temporal
dimension, they extract localized patterns and dependencies within the
sequence. This convolutional operation integrates information from
multiple timesteps, creating a more nuanced and structured
representation. After convolution, the output is reduced to a
lower-dimensional latent space, where temporal features are preserved
and compactly encoded. This method emphasizes localized temporal
dynamics while reducing dimensionality.

The \emph{Attention-based Autoencoder (AE\_err)}
combines convolutional layers with an attention mechanism to
reconstruct the input data, again using the convolutional layers to
capture local features while replacing LSTM with attention to capture
long-term patterns.

Fig.~\ref{fig:Fig2} illustrates this architecture. The input to the
encoder consists of a sequence of 240 measurements from two
simultaneous channels, along with a same-size sequence of time
representation. The input data passes through the encoder and is
compressed into a latent representation of size \( 1 \times 8 \),
where the first dimension represents a single compressed time step,
and the second dimension corresponds to eight learned features. This
is achieved by applying multi-head attention to the input data,
followed by a dropout layer for regularization, and a convolutional
layer for dimensionality reduction. The decoder then processes this
compressed representation using the reverse architecture (transposed
convolution, dropout, and attention) to reconstruct the original data,
capturing both trends and amplitudes.

\begin{figure}[tb]
    % \vskip -0.2in 
    \centering
    \includegraphics[width=1\columnwidth]{images/Fig2.png}  
    \caption{\label{fig:Fig2} AE\_err/AE\_att architecture.}
    % \vskip -0.0in 
\end{figure}

In all three autoencoding systems, anomaly detection is based on the
assumption that the compressed latent representation preserves only
recurring, periodic fluctuations and trends. Atypical spikes, often
caused by noise, do not cause enough loss to be worth the space to
represent them (in terms of nodes in the latent representation).
Therefore, the level of anomaly is estimated as the reconstruction
error, the difference between the original input sequence and the
sequence decoded from the latent representation.

The \emph{Transformer Predictor (TP\_err)} method utilizes a
Transformer that uses multi-head attention for time-series
forecasting. It consists of a sequence-to-sequence architecture, with
multi-head attention layers in both the encoder and decoder
components. The encoder captures temporal dependencies in the input
sequence, while the decoder predicts the next sequence based on these
encoded features.

Fig.~\ref{fig:Fig3} illustrates this architecture. The input consists
of a sequence of 240 measurements from two simultaneous channels,
along with a same-size sequence of time representation. This data
passes through the embedding layer, which applies multi-head attention
to capture key temporal patterns. The output is then processed with
dropout regularization and passed through a LeakyReLU activation
function to introduce non-linearity.


\begin{figure}[tb]
    % \vskip -0.2in 
    \centering
    \includegraphics[width=0.65\columnwidth]{images/Fig3.png}
    \caption{\label{fig:Fig3} TP\_err/TP\_att architecture.}
    % \vskip -0.0in 
\end{figure}

The TP\_err method estimates noise through prediction error, i.e. the
difference between the model's predicted output and observed
values. When the model predicts the next time step in the sequence, a
significant difference between the predicted and actual values
suggests that the input data may be noisy.



\subsection{Attention-Based Detection}

The exact same architectures as in AE\_err and TP\_err above are also used
for the respective attention-based detection methods
\emph{Attention-based Autoencoder (AbAE\_att)} and \emph{Transformer
Predictor (TP\_att)}, except that now anomaly is estimated by the
attention weights.

The key idea is that when a part of the sequence receives a low
attention weight, it suggests that the information at that moment is
likely noisy and, therefore, unimportant for the task.


\subsection{Conventional Approaches}

Alongside machine learning methods, conventional techniques provide a
reliable alternative for detecting noise in time-series data and are
commonly utilized. Among the various conventional methods proposed by
many researchers over the years, a statistical method and a
signal-processing technique have been considered for this work.

\emph{Principal Component Analysis (PCA)} is a linear dimensionality
reduction technique. It analyzes a data table where observations are
described by quantitative dependent inter-correlated variables. With
PCA, the important information is extracted from the table and
represented in a new set of orthogonal variables, called principal
components \cite{bro2014principal}. The similarity patterns of the
observations and the variables can then be depicted as points on maps
\cite{abdi2010principal}.

\emph{Multinomial Noise Exponential filtering (MNE)} is a method used
to remove unwanted frequencies from time-series data, typically EEG
signals. It applies a bandpass filter to the data, allowing signals
within a specific frequency range to pass through while reducing
frequencies outside this range. MNE Filtering is implemented using the
MNE python library, which provides tools for filtering and analyzing
neurophysiological data.




%%%%%%%%%%%%
\section{EXPERIMENT SETUP}
\label{sec:exp}
%%%%%%%%%%%%




The state-of-the-art machine learning and conventional methods serve as baseline methods against which attention-based methods are assessed.
Whereas conventional methods aim to detect noise through deviations of time-series data from estimated patterns, machine-learning methods follow an indirect approach. The considered models are trained to solve specific tasks---ranging from signal reconstruction and prediction to classification---and during inference, noise is detected in segments tied with higher reconstruction error, prediction error, or attention values.

To balance the models' focus on both the amplitude and trends of the time-series data, we define a custom loss function, called \emph{BlendedLoss}. This function combines the median and mean of the powered absolute differences between the predicted values ($\hat{x}$) and the target values ($x$):
%
\begin{equation}
\text{Loss} =
  (1 - b)\cdot\mathrm{median}(\lvert \hat{x} - x \rvert^p) +
  b\cdot\mathrm{mean}(\lvert \hat{x} - x \rvert^p)
\label{eq:blended_loss}
\end{equation}
%
where $p$ is the power parameter that controls the sensitivity of the
loss to differences and $b$ the \emph{blend factor} that controls
the trade-off between learning the overall trend (median error) and
closely following local patterns (mean error).
% A systematic
% investigation of various blend values within the range [0.1, 0.8]
% reveals the impact on the model's performance in data reconstruction.

Data normalization is required for attention-based and baseline machine-learning methods to ensure robust model training and reliable outcomes. The median and \emph{interquartile range (IQR)} are used instead of the mean and standard deviation, respectively, to improve robustness against outliers. The median measures central tendency and is resistant to extreme values,
% making it a more reliable indicator for datasets with low variability, where even small differences matter. Additionally, 
and IQR is a measure of statistical dispersion that represents the spread of the middle 50\% of a dataset and reduces the influence of outliers by focusing on the range within which the central portion of the data lies. Data scaling is performed using statistics derived from the entire dataset instead of relying on per-batch calculations. This approach ensures that the statistics are consistent across the full dataset, enhancing the reliability of the scaling process. 
% For a particular feature of a time-series $x$, its normalized value $x_{\text{norm}}$ is given:

% \begin{equation}
%     x_{\text{norm}} = \frac{x - \mathrm{median}(x)}{\mathrm{IQR}(x)} \label{eq:robust_norm}
% \end{equation}

% where $\mathrm{median}(x)$ and $\mathrm{IQR}(x)$ are the feature's median value and IQR respectively.

In the case of conventional methods, no normalization is applied to data since these methods require raw data that preserves the characteristics of the original signal.





This study employs a case analysis to validate the efficacy of the proposed noise estimation methods. The focus is detecting noise within electroencephalographic (EEG) signals, recorded via headbands utilized during slumber to monitor various stages of the user's sleep cycle.
\subsection{Test case description} One of the main concerns when dealing with electroencephalographic signals (EEG) is assuring that clean data with a high signal-to-noise ratio is recorded. The EEG signal amplitude is in the microvolts range, and it is easily contaminated with noise, known as artifacts, which need to be filtered from the neural processes to keep the valuable information needed for different applications.

In this domain, an artifact is denoted as any component of the EEG signal not directly produced by human brain activity, making the system register noise that contaminates the neural EEG data. The ability to recognize these artifacts is the first step in removing them. EEG artifacts can be classified depending on their origin, which can be physiological or external to the human body (technical/non-physiological). Fig.~\ref{fig:Fig1} shows some examples of EEG signals contaminated with noise.

% \begin{figure}
%     % \vskip -0.2in 
%     \centering
%     \includegraphics[width=0.90\columnwidth]{images/Fig1n.png}     
%     \vspace{0.15em}
%     \caption{\label{fig:Fig1} Examples of the temporal visualization of EEG signals and coloring of noisy segments.}
%     % \vskip -0.0in 
% \end{figure}

\begin{figure}[tb]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/bb1.png}     
    \end{minipage}
    \vspace{0.05cm}
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/bb2.png}     
    \end{minipage}
    \vspace{0.05cm}
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/bb3.png} 
    \end{minipage}
    \vspace{0.05cm}
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/bb4.png}
    \end{minipage}
    \caption{\label{fig:Fig1} Examples of the temporal visualization of EEG signals and coloring of noisy segments.}
\end{figure}


In this figure, pink-colored areas are typical of low-frequency noise (0.2-4Hz), which is mainly due to perspiration originating from small drops of sweat produced by the skin glands, which cause changes in the electrical baseline of the electrodes. Brown-colored and green-colored areas are typical of high-frequency noise (30-45Hz) that may originate from electrical activity produced by the muscles when they are contracted, like, for example, muscle tension in the jaw or forehead that can take place when clenching or frowning, respectively. Orange-colored areas are typical of high-amplitude noise that may be due to temporary failures in contact between the EEG sensor and the scalp produced by touching the sensor or by spontaneous changes in electrode-skin contact.

In this analysis, EEG signals collected from a headband with two EEG channels synchronized with medical-grade EEG devices are considered. The recorded signal frequency for each channel of the headband is 128Hz. These signals are grouped into consecutive 30-second segments, and each segment is further annotated by three experts, into one of five sleep stages: Wake, N1, N2, N3, and REM. These stages correspond to specific brain activity patterns, such as slow eye movements, sleep spindles, and other characteristic waveforms. The goal of the current analysis is to detect the presence of noise, if any, at any of these segments. In total, 56 recordings were examined. Each recording refers to the EEG signals acquired by the headband of a user during a night-long sleep. These signals are grouped into 30-second consecutive segments.

As a ground-truth method to evaluate the results of the proposed noise estimation methods, the original estimation method used by the headband manufacturer is considered. This method consists of task-specific algorithms to automatically estimate noise and evaluate the overall quality of EEG signals, identifying the artifacts in recordings made using the wearable textile headband.

\subsection{Noise estimation methods training and set-up}

To train the machine learning models, the available recordings were
split into training (43 recordings), validation (3 recordings), and
testing subsets (10 recordings). Noise estimation is performed using
the test dataset. For the training configuration, we set the batch
size to 512 and trained the models for a maximum of 1000 epochs. To
prevent overfitting, we employ early stopping with patience of 30
epochs, meaning that if the validation loss does not improve for 30
consecutive epochs, training will stop. We set the learning rate to
1e-4 and use the Adam optimizer to adjust the model
weights. Additionally, we implement a ReduceLROnPlateau scheduler to
adjust the learning rate dynamically based on the validation loss. If
the validation loss plateaus, the scheduler reduces the learning rate
to help the model continue improving.

To process the input data, we split each 30-second segment of EEG
data---that consists of 7,680 EEG records per channel---into 32
smaller consecutive chunks, each consisting of a sequence of 240 EEG
records per channel. This structure allows the model to focus on both
the short and long-term temporal dependencies within the
data. Additionally, since the available dataset tends to overfit with
more complex models due to its low variability and extreme outliers,
simple architectures with only a few layers, e.g. a single attention
layer in the attention-based models, where considered. This approach
avoids unnecessary complexity, ensuring more stable training and
better generalization on the data.






The machine-learning methods, as well as the statistical method MNE filtering, implemented in this study are publicly available \href{https://github.com/your-repo}{here} to ensure reproducibility and support further research. Regarding the statistical method PCA, it is implemented using the Frouros\footnote{Documentation and source code are available \href{https://github.com/IFCA-Advanced-Computing/frouros}{here}. Access to the specific code designed and parametrized for this paper can be discussed with the authors from Four Dot Infinity upon request.} open-source Python library for drift detection in machine learning systems.


%%%%%%%%%%%%
\section{Evaluation and Results}
\label{sec:results}
%%%%%%%%%%%%

Table~\ref{tab2} presents the number of artifacts identified through
the estimation methodology employed by the headband manufacturer
across the ten recordings utilized to assess the anomaly detection
techniques, arranged in descending order of the true-error rate based on the HB2 measurements. Specifically, it delineates the total count of 30-second
segments within which a minimum of one artifact is detected by the
headband manufacturer for each of the ten recordings, as recorded by
each EEG channel (designated as HB1 and HB2). The true-error rate is the percentage of segments found with artifacts among all segments of the corresponding recording, and in the remaining of the paper acts as a recording identifier. 


\begin{table}[bt]
\caption{Absolute number and percentage of segments with artifacts in
  each channel and recording session.}
\centering
\renewcommand{\arraystretch}{1.3} % Adjust row height
\begin{tabular}{lrrrr}
\toprule
RecID &  A         &  B       &  C & D \\
\midrule
HB1   &   0        & 57 (5\%) & 27 (3\%) & 9 (1\%) \\

HB2   & 151 (14\%) & 56 (5\%) & 26 (3\%) & 9 (1\%) \\
\bottomrule
\end{tabular}
\label{tab2}
\end{table}

The table illustrates that in 3 out of 10 recordings, both channels detect no artifacts. In another 3 recordings, only one segment is found contaminated with noise. In three recordings, noisy segments are almost unanimously detected by both channels. Finally, in one recording, only the HB2 channel identifies noise in several segments, suggesting a potential issue with the corresponding sensor.


\input{tab_results}


Table~\ref{tab3} presents the F2 score, together with the Precision and Recall metrics (shown in parenthesis, respectively), for the evaluated methods across the four test recordings, where the manufacturer detected more than one artifact.
The Precision metric in Table~\ref{tab3} denotes the percentage of
actual segments identified as having artifacts relative to the total
segments predicted to contain artifacts. However, this metric fails to
convey information regarding missed artifacts---specifically, those not
identified by the anomaly detection methodologies. This aspect is
quantified through the Recall metric (Table~\ref{tab4}), which
indicates the ratio of segments predicted to have artifacts against
the total number of actual segments containing artifacts. For
instance, the PCA method exhibits a precision of 100\% from the data
obtained during the HB1 test recording 1, yet it only achieves a
recall of 11.11\%. This discrepancy arises because the PCA method
successfully detected a singular artifact characterized by noise,
deemed a noisy artifact; thus, the precision remains at
100\%. However, it failed to identify the additional eight noisy
segments (refer to Table~\ref{tab2}), resulting in a significantly
reduced recall of approximately 11\%. Instances where there are empty
cells, signify scenarios where the respective method did not identify
any artifacts. Consequently, precision cannot be ascertained, and
recall is recorded as 0\%.

In this application, our primary objective is to identify as many segments as possible that contain artifacts while allowing for some tolerance regarding detecting noisy segments. The principal aim should be to reduce the number of instances requiring the application of reliable yet non-automated anomaly detection methods. Consequently, the key performance metric to prioritize is Recall. For this reason, the F2 score is considered, which is a weighted harmonic mean of precision and recall, and where recall is given more importance than precision.

From the results presented in Table~\ref{tab3}, we conclude that all considered methods exhibited suboptimal performance. However, PCA demonstrated relatively high recall values in two recordings. One plausible explanation for this observation could be the selected time-window of 30 seconds that defines the examined segment. The efficacy of anomaly detection methods in time series analysis may be significantly influenced by the length of the time series, as supported by existing literature \cite{Lee_2021}.

Due to this reasoning, we applied the selected methodologies to the same dataset while extending the segment duration from 30 seconds to 5 and 10 minutes. Table~\ref{tab4} and Table~\ref{tab5} present the resulting F2 scores, Precision, and Recall values of the selected methods for the 5 and 10-minute windows, respectively. 

One initial conclusion drawn is that across all methods analyzed, the detection of anomalies is significantly enhanced when the duration of the time-window assigned to each segment is increased. However, this improvement comes at the expense of broader time-windows during which the detected anomalies may actually occur---a consideration that necessitates careful evaluation. 
The primary conclusions are drawn through a comparative analysis of the performance of the evaluated methodologies. The introduced concept of utilizing attention mechanisms as a critical metric for anomaly detection appears to be promising. Across all segment durations (30 seconds, 5 minutes, and 10 minutes), the F2 and Recall metrics for the AbAE\_att and TP\_att methods, which identify anomalies based on their attention matrix values, surpass those of their counterparts, AbAE\_err and TP\_err, which detect anomalies via reconstruction or prediction errors, in all but one recording.
The performance of attention-based methods is generally comparable to, if not superior, that of other state-of-the-art techniques, such as LSTM-AE and C-LSTM-AE. A similar conclusion can be derived from comparing the F2 scores and Recall values of the examined attention-based methods with those of baseline methods, including PCA and MNE. However, it is evident that the TP\_att method significantly outperforms the AbAE\_att method.



\section{Conclusions and Future Work}
\label{sec:conc}
This paper investigates the innovative application of attention mechanisms within machine learning models as pivotal indicators for anomaly detection in time-series datasets. Two methodologies, namely an Autoencoder and a Transformer, are examined, with their performances benchmarked against conventional techniques and other advanced machine-learning approaches. The efficacy of the proposed methodology is assessed utilizing EEG signals sourced from wearable sleep-monitoring devices measuring Precision and Recall against a manufacturer's established ground truth methodology. The findings indicate that attention-based approaches, particularly the Transformer Predictor, exhibit promising capabilities in identifying anomalies, particularly with extended time windows for analysis. The study concludes that attention mechanisms significantly enhance anomaly detection in streaming data environments.

The current study acts as a proof of concept, illustrating the potential of attention mechanisms as key-indicators of anomalies in time series data. Subsequent steps will entail exploring additional architectures beyond the AbAE\_att and TP\_att methods analyzed herein. Additionally, the investigation will focus on the performance parameters of attention-based techniques, such as the choice of time-window segments, to develop a task-agnostic approach for anomaly detection.



\ifnum\anon=0

%%%%%%%%%%%%
\section*{Acknowledgment}
%%%%%%%%%%%%

This research was co-funded by the European Union under GA no. 101135782 (MANOLO project). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or CNECT. Neither the European Union nor CNECT can be held responsible for them. AWS resources were provided by the National Infrastructures for Research and Technology GRNET and funded by the EU Recovery and Resiliency Facility.

\fi

% \section*{References}

\bibliographystyle{IEEEtranDOI}
\bibliography{refs}

\end{document}
